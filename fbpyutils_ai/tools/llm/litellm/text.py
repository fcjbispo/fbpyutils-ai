import os
import litellm
from typing import Dict, List
from fbpyutils_ai import logging

litellm.logging = logging
litellm.drop_params = True


def _generate_text(
    self,
    prompt: str,
    base_type: str = "base",
    **kwargs,
) -> str:
    """
    Generates text from a prompt using the OpenAI API.

    Args:
        prompt (str): The prompt sent for text generation.
        base_type (str): The type of model to use.
        **kwargs: Additional parameters for the request.

    Returns:
        str: Text generated by the API.
    """
    try:
        if not prompt or len(prompt) == 0:
            raise ValueError("Prompt cannot be empty.")

        base_type = base_type or "base"
        kwargs["timeout"] = kwargs.get("timeout", self.timeout)
        kwargs["stream"] = kwargs.get("stream", False)

        provider = self.model_map[base_type].provider
        os.environ[f"{provider.upper()}_API_BASE"] = self.model_map[base_type].api_base_url
        os.environ[f"{provider.upper()}_API_KEY"] = self.model_map[base_type].api_key
        response = litellm.text_completion(
            model=self._resolve_model(base_type),
            prompt=[prompt],
            **kwargs,
        )
        if response:
            if response.get("choices", [{}])[0].get("text", None):
                return response["choices"][0]["text"]
            else:
                raise ValueError(f"Invalid model response: {response}.")
        else:
            raise ValueError(f"Invalid model response format: {type(response)}.")
    except Exception as e:
        logging.error(f"Invalid model provider response: {e} - {response}")
        print(e)
        return None


def generate_text(
    self,
    prompt: str,
    **kwargs,
) -> str:
    return _generate_text(self, prompt, **kwargs)
