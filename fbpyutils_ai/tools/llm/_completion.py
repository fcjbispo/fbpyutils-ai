# fbpyutils_ai/tools/llm/_completion.py
from typing import List, Dict

from fbpyutils_ai import logging

# Note: These functions assume they will be bound to an instance of OpenAITool
# and will have access to `self._make_request`, `self.api_base`, `self.model_id`, etc.

def generate_text(
    self,
    prompt: str,
    max_tokens: int = 300,
    temperature: float = 0.8,
    vision: bool = False,
) -> str:
    """
    Generates text from a prompt using the appropriate API endpoint (vision or base).

    Args:
        self: The instance of OpenAITool.
        prompt (str): The prompt sent for text generation.
        max_tokens (int): Maximum number of tokens to generate.
        temperature (float): Sampling temperature.
        vision (bool): If True, uses the vision API endpoint and model.

    Returns:
        str: Text generated by the API, or an empty string on error.
    """
    api_base = self.api_vision_base if vision else self.api_base
    api_key_to_use = self.api_vision_key if vision else self.api_key
    model_id = self.vision_model if vision else self.model_id
    logging.info(f"Generating text using model {model_id} at {api_base}. Vision mode: {vision}.")

    tokens = max_tokens or 300
    headers = self.api_headers.copy() # Use copy to avoid modifying class headers

    # Adjust Authorization header if necessary (e.g., if vision key differs and not using Anthropic key)
    # This logic assumes non-Anthropic providers might use Bearer tokens for different keys
    is_anthropic = 'api.anthropic.com' in api_base.lower()
    if not is_anthropic and api_key_to_use != self.api_embed_key: # Check if key differs from the default embed key in headers
         headers["Authorization"] = f"Bearer {api_key_to_use}"
         logging.debug("Adjusted Authorization header for non-embedding API key.")
    elif is_anthropic and "x-api-key" in headers and headers["x-api-key"] != api_key_to_use:
        # Ensure correct key is used for Anthropic if vision/base key differs from main key
        headers["x-api-key"] = api_key_to_use
        logging.debug("Adjusted x-api-key header for Anthropic non-embedding API key.")


    data = {
        "model": model_id,
        "prompt": prompt, # Be cautious logging full prompts if they contain sensitive data
        "max_tokens": tokens,
        "temperature": temperature,
    }
    logging.debug(f"Request data (prompt omitted for brevity): { {k: v for k, v in data.items() if k != 'prompt'} }")

    # Determine the correct endpoint path (might vary slightly between providers)
    # Assuming OpenAI-like structure for now
    endpoint_path = "/completions" # Older endpoint, might need /v1/completions
    if "/v1" not in api_base: # Simple check, might need refinement
        api_base = api_base.rstrip('/') + "/v1"

    url = f"{api_base.rstrip('/')}{endpoint_path}"
    logging.debug(f"Text generation request URL: {url}")


    try:
        result = self._make_request(url, headers, data)
        # Handle potential variations in response structure
        if result and "choices" in result and result["choices"]:
            choice = result["choices"][0]
            if "text" in choice: # Standard completion format
                 generated_text = choice["text"].strip()
                 logging.info(f"Successfully generated text (length: {len(generated_text)}).")
                 return generated_text
            else:
                 logging.warning(f"Unexpected response format: 'text' key missing in choice. Choice: {choice}")
                 return ""
        else:
             logging.warning(f"Unexpected response format: 'choices' key missing or empty. Response: {result}")
             return ""
    except (KeyError, IndexError, TypeError) as e:
        # Log the result if available, otherwise just the error
        response_data = locals().get("result", "N/A")
        logging.error(f"Error parsing text completion response: {e}. Response: {response_data}", exc_info=True)
        return ""
    except Exception as e:
        logging.error(f"Failed to generate text completion: {e}", exc_info=True)
        return ""


def generate_completions(
    self, messages: List[Dict[str, str]], model_id: str = None, **kwargs
) -> str:
    """
    Generates a chat response from a list of messages using the chat completions API endpoint.

    Args:
        self: The instance of OpenAITool.
        messages (List[Dict[str, str]]): Conversation history.
        model_id (str, optional): Model ID to use. Defaults to self.model_id.
        **kwargs: Additional parameters for the API request (e.g., temperature, max_tokens).

    Returns:
        str: The generated chat message content, or an empty string on error.
    """
    effective_model_id = model_id or self.model_id
    logging.info(f"Generating chat completion using model {effective_model_id}.")

    headers = self.api_headers.copy()
    # Adjust auth header if base API key differs from embed key and not Anthropic
    is_anthropic = 'api.anthropic.com' in self.api_base.lower()
    if not is_anthropic and self.api_key != self.api_embed_key:
        headers["Authorization"] = f"Bearer {self.api_key}"
        logging.debug("Adjusted Authorization header for base API key.")
    elif is_anthropic and "x-api-key" in headers and headers["x-api-key"] != self.api_key:
        headers["x-api-key"] = self.api_key
        logging.debug("Adjusted x-api-key header for Anthropic base API key.")


    data = {"model": effective_model_id, "messages": messages, **kwargs}
    logging.debug(f"Request data (messages omitted for brevity): { {k: v for k, v in data.items() if k != 'messages'} }")

    # Determine the correct endpoint path
    endpoint_path = "/chat/completions" # Standard OpenAI path
    api_base = self.api_base.rstrip('/')
    if "/v1" not in api_base: # Simple check, might need refinement
        api_base += "/v1"

    url = f"{api_base}{endpoint_path}"
    logging.debug(f"Chat completion request URL: {url}")

    try:
        result = self._make_request(url, headers, data)
        logging.debug(f"Raw chat completion response: {result}")
        if (
            result and "choices" in result
            and result["choices"]
            and "message" in result["choices"][0]
            and "content" in result["choices"][0]["message"]
        ):
            content = result["choices"][0]["message"]["content"]
            # Content might be None, handle appropriately
            content_str = content.strip() if content else ""
            logging.info(f"Successfully generated chat completion (length: {len(content_str)}).")
            return content_str
        else:
            logging.warning(f"Could not extract content from chat response. Response: {result}")
            return ""
    except (KeyError, IndexError, TypeError) as e:
        response_data = locals().get("result", "N/A")
        logging.error(f"Error parsing chat completion response: {e}. Response: {response_data}", exc_info=True)
        return ""
    except Exception as e:
        logging.error(f"Failed to generate chat completion: {e}", exc_info=True)
        return ""
