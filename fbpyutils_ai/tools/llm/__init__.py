from abc import abstractmethod
import json
import os
import threading
from typing import List, Optional, Dict, Any, Union, Generator
from jsonschema import ValidationError, validate
from tenacity import retry, wait_random_exponential, stop_after_attempt

from fbpyutils_ai.tools import LLMService, LLMServiceModel
from fbpyutils_ai.tools.http import RequestsManager, basic_header
from fbpyutils_ai.tools.llm.utils import get_api_model_response, get_llm_resources
from fbpyutils_ai import logging


(
    LLM_PROVIDERS,
    LLM_COMMON_PARAMS,
    LLM_INTROSPECTION_PROMPT,
    LLM_INTROSPECTION_VALIDATION_SCHEMA,
) = get_llm_resources()


class OpenAITool(LLMService):
    _request_semaphore = threading.Semaphore(int(os.environ.get("FBPY_SEMAPHORES", 4)))

    def __init__(
        self,
        base_model: LLMServiceModel,
        embed_model: Optional[LLMServiceModel] = None,
        vision_model: Optional[LLMServiceModel] = None,
        timeout: int = 300,
        retries: int = 3,
    ):
        super().__init__(base_model, embed_model, vision_model, timeout, retries)
        self.session = RequestsManager.create_session()
        self.session.headers.update(basic_header())

    def _resolve_model(self, map_type: str = "base") -> LLMServiceModel:
        try:
            model = self.model_map[map_type]
            return f"{model.provider}/{model.model_id}"
        except KeyError:
            return self._resolve_model()

    @retry(
        wait=wait_random_exponential(multiplier=1, max=30), stop=stop_after_attempt(3)
    )
    def _make_request(
        self,
        url: str,
        headers: Dict[str, str],
        json_data: Dict[str, Any],
        timeout: int,
        stream: bool = False,
        method: str = "POST",
    ) -> Union[Dict[str, Any], Generator[Dict[str, Any], None, None]]:
        """
        Wrapper around RequestsManager.make_request that applies the semaphore for rate limiting.

        Args:
            url: The URL to make the request to
            headers: The headers to include in the request
            json_data: The JSON data to include in the request body
            stream: Whether to stream the response
            method: The HTTP method to use for the request

        Returns:
            If stream=False, returns the JSON response as a dictionary.
            If stream=True, returns a generator yielding parsed JSON objects from the streaming response.
        """
        headers = headers or self.session.headers
        json_data = json_data or {}
        method = (method or "POST").upper()
        if method not in ["GET", "POST"]:
            raise ValueError(f"Unsupported HTTP method: {method}")
        with OpenAITool._request_semaphore:
            return RequestsManager.make_request(
                session=self.session,
                url=url,
                headers=headers,
                json_data=json_data,
                timeout=timeout or self.timeout,
                method=method,
                stream=stream,
            )

    @abstractmethod
    def generate_embeddings(self, input: List[str], **kwargs) -> Optional[List[float]]:
        """
        Generates an embedding for a given text using the OpenAI API.

        Args:
            text (str): Text for which the embedding will be generated.
            **kwargs: Additional parameters for the request.

        Returns:
            Optional[List[float]]: List of floats representing the embedding or None in case of an error.
        """
        pass

    @abstractmethod
    def generate_text(
        self,
        prompt: str,
        **kwargs,
    ) -> str:
        """
        Generates text from a prompt using the OpenAI API.

        Args:
            prompt (str): The prompt sent for text generation.
            **kwargs: Additional parameters for the request.

        Returns:
            str: Text generated by the API.
        """
        pass

    @abstractmethod
    def generate_completions(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """
        Generates a chat response from a list of messages using the OpenAI API.

        Args:
            messages (List[Dict[str, str]]): List of messages that make up the conversation.
            **kwargs: Additional parameters for the request.

        Returns:
            str: Response generated by the API.
        """
        pass

    @abstractmethod
    def generate_tokens(self, text: str) -> List[int]:
        """
        Generates a list of tokens from a text using the tiktoken library,
        compatible with OpenAI models.

        Args:
            text (str): The text to be tokenized.

        Returns:
            List[int]: List of tokens generated from the text.
        """
        pass

    @abstractmethod
    def describe_image(self, image: str, prompt: str, **kwargs) -> str:
        """
        Describes an image using the OpenAI API, combining a prompt with the image content.
        The image can be provided as:
            - Path to a local file,
            - Remote URL,
            - or a string already encoded in base64.

        The method converts the image to base64 (when necessary) and adds this information to the prompt,
        which is sent to the configured model to generate a description.

        Args:
            image (str): Path to the local file, remote URL, or base64 content of the image.
            prompt (str): Prompt that guides the image description.
            max_tokens (int, optional): Maximum number of tokens to be generated. Default is 300.
            temperature (int, optional): Temperature of text generation. Default is 0.4.

        Returns:
            str: Description generated by the OpenAI API for the image.
        """
        pass

    @staticmethod
    def get_providers() -> List[Dict[str, Any]]:
        """Lists the available providers."""
        return LLM_PROVIDERS

    @staticmethod
    def list_models(
        api_base_url: str, api_key: str, **kwargs: Any
    ) -> List[Dict[str, Any]]:
        """
        Retrieves a structured list of all available LLM provider models.

        Args:
            api_base_type (str, optional): Specifies the API endpoint type. Options are "base", "embed_base", or "vision_base".
                Defaults to "base".

        Returns:
            List[Dict[str, Any]]: A list of dictionaries containing model metadata.

        Raises:
            requests.exceptions.RequestException: If there is an error communicating with the API.

        Usage Examples:
            >>> tool = OpenAITool(model="text-davinci-003", api_key="your_api_key")
            >>> models = tool.list_models(api_base_type="base")
            >>> print(models)

            >>> models = tool.list_models(api_base_type="embed_base")
            >>> print(models)

            >>> models = tool.list_models(api_base_type="vision_base")
            >>> print(models)
        """
        if not all([api_base_url, api_key]):
            raise ValueError("api_base_ur and api_key must be provided.")

        url = f"{api_base_url}/models"
        models_data = get_api_model_response(url, api_key, **kwargs)

        # Parse and structure the model metadata
        if not url.endswith("/models"):
            return models_data

        models = []
        if "data" in models_data:
            models_data = models_data.get("data", [])
        for model in models_data:
            models.append(model)

        return models

    @staticmethod
    def get_model_details(
        provider: str,
        api_base_url: str,
        api_key: str,
        model_id: str,
        **kwargs: Any,
    ) -> Dict[str, Any]:
        """Gets the details of a model."""
        if not all([provider, api_base_url, api_key]):
            raise ValueError("provider, api_base_ur, and api_key must be provided.")

        kwargs["timeout"] = kwargs.get("timeout", 300)
        kwargs["retries"] = kwargs.get("retries", 3)
        response_data = {}
        try:
            url = f"{api_base_url}/models/{model_id}"

            if provider == "openrouter":
                url += "/endpoints"

            logging.info(f"Fetching model details from: {url}")
            response_data = get_api_model_response(url, api_key, **kwargs)

            return response_data
        except Exception as e:
            logging.error(
                f"Failed to retrieve model details: {e}. Response data: {response_data}"
            )
            raise
